import numpy as np
import random
from IPython.display import clear_output
import SOMPiBrain

class HanoiEnv:
    def __init__(self, n_discs):
        self.n = n_discs
        self.st = np.zeros((self.n, 3), dtype=int)
        for i in range(self.n, 0, -1):
            self.st[i - 1, 0] = i
        self.count = 0
        self.dumm = 0

    def reset(self):
        self.__init__(self.n)
        return self.st.flatten()

    def move(self, her, zil):
        if her == zil:
            self.dumm += 1
            print("so kann man auch Zeit verbringen...\nNeudumm = ", self.dumm)
            return

        i = 0
        while i < self.n and self.st[i, her] == 0:
            i += 1

        if i == self.n:
            self.dumm += 1
            print("da is nix, mach nochmal\nNeudumm = ", self.dumm)
            return

        print("Herkunftsspalte, Eintrag in Zeile i: ", i)

        if np.sum(self.st[:, zil]) == 0:
            self.st[self.n - 1, zil] = self.st[i, her]
            self.st[i, her] = 0
            self.count += 1
        else:
            j = 0
            while j < self.n and self.st[j, zil] == 0:
                j += 1

            if j == self.n or self.st[j, zil] < self.st[i, her]:
                self.dumm += 1
                print("du schummler willst ja schummeln, aber das geht nich!!!!\nNeudumm = ", self.dumm)
                return
            else:
                self.st[j - 1, zil] = self.st[i, her]
                self.st[i, her] = 0
                self.count += 1

        print(self.st, "\nnach Zug Nr. ", self.count)
        self.sieg()  # hab ich schon gewonnen?

    def sieg(self):
        return np.sum(self.st[:, 0]) + np.sum(self.st[:, 1]) == 0

# Discretize the state space by digitizing the values
import numpy as np

def discretize_state(state):
    if state is None:
        # Handle the case where state is None
        return 0
    else:
        # Your existing discretization logic
        return np.digitize(state, np.linspace(1, 3, 4))

# Example usage:
# next_state = discretize_state(next_state)

env = HanoiEnv(n_discs=3)
state_num = 3 * 3 * 2  # Discretized state space
action_num = 9  # Assuming 3 pegs and 3 possible moves for each peg
brain = SOMPiBrain.SOMPiBrain(state_num, action_num)

print("Training started.\n")

for episode in range(1, 21):
    state = env.reset()
    state = discretize_state(state)
    epochs = 0
    reward = 0
    done = False

    while not done:
        action = brain.get_action(state)
        next_state = env.move(action // 3, action % 3)
        next_state = discretize_state(next_state)
        state = brain.reward_action(state, next_state, action, reward)
        epochs += 1
        done = env.sieg()

    if episode % 10000 == 0:
        clear_output(wait=True)
        print(f"Episode: {episode}")

print("Training finished.\n")

# Evaluation (customize based on your criteria)
print("Evaluate agent's performance.\n")
state = env.reset()
state = discretize_state(state)
epochs, penalties, reward = 0, 0, 0
done = False

while not done:
    action = brain.get_action(state, False)
    next_state = env.move(action // 3, action % 3)
    next_state = discretize_state(next_state)
    state = next_state
    epochs += 1
    done = env.sieg()

print(f"Results after evaluation:")
print(f"Total moves taken: {epochs}")
